{\rtf1\ansi\ansicpg936\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset134 PingFangSC-Regular;\f2\fnil\fcharset0 Menlo-Regular;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red0\green0\blue0;\red89\green138\blue67;
\red202\green202\blue202;\red183\green111\blue179;\red67\green192\blue160;\red212\green214\blue154;\red70\green137\blue204;
\red140\green211\blue254;\red167\green197\blue152;\red194\green126\blue101;\red66\green179\blue255;\red183\green111\blue179;
\red202\green202\blue202;\red67\green192\blue160;\red140\green211\blue254;\red212\green214\blue154;\red66\green179\blue255;
\red167\green197\blue152;\red89\green138\blue67;\red70\green137\blue204;\red194\green126\blue101;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgray\c0\c0;\cssrgb\c41569\c60000\c33333;
\cssrgb\c83137\c83137\c83137;\cssrgb\c77255\c52549\c75294;\cssrgb\c30588\c78824\c69020;\cssrgb\c86275\c86275\c66667;\cssrgb\c33725\c61176\c83922;
\cssrgb\c61176\c86275\c99608;\cssrgb\c70980\c80784\c65882;\cssrgb\c80784\c56863\c47059;\cssrgb\c30980\c75686\c100000;\cssrgb\c77255\c52549\c75294;
\cssrgb\c83137\c83137\c83137;\cssrgb\c30588\c78824\c69020;\cssrgb\c61176\c86275\c99608;\cssrgb\c86275\c86275\c66667;\cssrgb\c30980\c75686\c100000;
\cssrgb\c70980\c80784\c65882;\cssrgb\c41569\c60000\c33333;\cssrgb\c33725\c61176\c83922;\cssrgb\c80784\c56863\c47059;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 \
1.28\

\f1 \'b5\'a5-\'b5\'a5\'c2\'b7\'be\'b6\'b9\'e6\'bb\'ae\
\pard\pardeftab720\partightenfactor0

\f2 \cf2 \expnd0\expndtw0\kerning0
# 
\f1 \'c9\'ee\'b6\'c8\'c7\'bf\'bb\'af\'d1\'a7\'cf\'b0
\f2 \'97\'97
\f1 \'d4\'ad\'c0\'ed\'a1\'a2\'cb\'e3\'b7\'a8\'d3\'eb
\f2 PyTorch
\f1 \'ca\'b5\'d5\'bd\'a3\'ac\'b4\'fa\'c2\'eb\'c3\'fb\'b3\'c6\'a3\'ba\'b4\'fa
\f2 35-Double DQN
\f1 \'cb\'e3\'b7\'a8\'ca\'b5\'d1\'e9
\f2 .py\
\
\pard\pardeftab720\partightenfactor0
\cf2 import gym, random, pickle, os.path, math, glob\
\
import numpy as np\
import pandas as pd\
import matplotlib.pyplot as plt\
\
import torch\
import torch.optim as optim\
import torch.nn as nn\
import torch.nn.functional as F\
import torch.autograd as autograd\
import pdb\
\
\pard\pardeftab720\partightenfactor0
\cf2 # from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\
\pard\pardeftab720\partightenfactor0
\cf2 from IPython.display import clear_output\
from tensorboardX import SummaryWriter\
\
\pard\pardeftab720\partightenfactor0
\cf2 class DQN(nn.Module):\
    def __init__(self, in_channels=4, num_actions=5):\
        """\
        Initialize a deep Q-learning network as described in\
        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\
        Arguments:\
            in_channels: number of channel of input.\
                i.e The number of most recent frames stacked together as describe in the paper\
            num_actions: number of action-value to output, one-to-one correspondence to action in game.\
        """\
        super(DQN, self).__init__()\
        self.fc1 = nn.Linear(in_channels, 100)\
        self.fc1.weight.data.normal_(0, 0.1)\
        self.fc2 = nn.Linear(100, 100)\
        self.fc2.weight.data.normal_(0, 0.1)\
        self.out = nn.Linear(100, num_actions)\
        self.out.weight.data.normal_(0, 0.1)\
\
    def forward(self, x):                                                       # 
\f1 \'b6\'a8\'d2\'e5
\f2 forward
\f1 \'ba\'af\'ca\'fd
\f2  (x
\f1 \'ce\'aa\'d7\'b4\'cc\'ac
\f2 )\
        y = F.relu(self.fc1(x))                                                 # 
\f1 \'c1\'ac\'bd\'d3\'ca\'e4\'c8\'eb\'b2\'e3\'b5\'bd\'d2\'fe\'b2\'d8\'b2\'e3\'a3\'ac\'c7\'d2\'ca\'b9\'d3\'c3\'bc\'a4\'c0\'f8\'ba\'af\'ca\'fd
\f2 ReLU
\f1 \'c0\'b4\'b4\'a6\'c0\'ed\'be\'ad\'b9\'fd\'d2\'fe\'b2\'d8\'b2\'e3\'ba\'f3\'b5\'c4\'d6\'b5
\f2 \
        z = F.relu(self.fc2(y))\
        actions_value = self.out(z)                                             # 
\f1 \'c1\'ac\'bd\'d3\'d2\'fe\'b2\'d8\'b2\'e3\'b5\'bd\'ca\'e4\'b3\'f6\'b2\'e3\'a3\'ac\'bb\'f1\'b5\'c3\'d7\'ee\'d6\'d5\'b5\'c4\'ca\'e4\'b3\'f6\'d6\'b5
\f2  (
\f1 \'bc\'b4\'b6\'af\'d7\'f7\'d6\'b5
\f2 )\
        return actions_value                                                    # 
\f1 \'b7\'b5\'bb\'d8\'b6\'af\'d7\'f7\'d6\'b5
\f2 \
\
class Memory_Buffer(object):\
    def __init__(self, memory_size=1000):\
        self.buffer = []\
        self.memory_size = memory_size\
        self.next_idx = 0\
\
    def push(self, state, action, reward, next_state, done):\
        data = (state, action, reward, next_state, done)\
        if len(self.buffer) <= self.memory_size: # buffer not full\
            self.buffer.append(data)\
        else: # buffer is full\
            self.buffer[self.next_idx] = data\
        self.next_idx = (self.next_idx + 1) % self.memory_size\
\
    def sample(self, batch_size):\
        states, actions, rewards, next_states, dones = [], [], [], [], []\
        for i in range(batch_size):\
            idx = random.randint(0, self.size() - 1)\
            data = self.buffer[idx]\
            state, action, reward, next_state, done= data\
            states.append(state)\
            actions.append(action)\
            rewards.append(reward)\
            next_states.append(next_state)\
            dones.append(done)\
\
\
        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\
\
    def size(self):\
        return len(self.buffer)\
\
class DDQNAgent:\
    def __init__(self, in_channels = 1, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\
        self.epsilon = epsilon\
        self.action_space = action_space\
        self.memory_buffer = Memory_Buffer(memory_size)\
        self.DQN = DQN(in_channels = in_channels, num_actions = action_space)\
        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space)\
        self.DQN_target.load_state_dict(self.DQN.state_dict())\
\
\
        self.USE_CUDA = USE_CUDA\
        if USE_CUDA:\
            self.DQN = self.DQN.cuda()\
            self.DQN_target = self.DQN_target.cuda()\
        #self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\
        self.optimizer = optim.Adam(self.DQN.parameters(),lr=lr, eps=0.001, betas=(0.9,0.99))\
\
    def observe(self, lazyframe):\
        # from Lazy frame to tensor\
        state =  torch.from_numpy(lazyframe).float()\
        if self.USE_CUDA:\
            state = state.cuda()\
        return state\
\
    def value(self, state):\
        q_values = self.DQN(state)\
        return q_values\
\
    def act(self, state, epsilon = None):\
        """\
        sample actions with epsilon-greedy policy\
        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\
        """\
        if epsilon is None: epsilon = self.epsilon\
        action=np.zeros([N,action_number],int)\
        q_values = self.value(state).cpu().detach().numpy()\
        if random.random()<epsilon:\
            #aciton = random.randrange(self.action_space)\
            for i in range(N):#torch.squeeze(actions_value,0).data.numpy().reshape(N,5):\
                action[i,int(np.random.randint(0, action_number))] = 1\
        else:\
            actions_value = torch.from_numpy(q_values.reshape(N,action_number))\
            for i in range(N):\
                a = torch.unsqueeze(actions_value[i], 0)\
                temp = torch.max(a,1)[1].data.numpy()\
                action[i,int(temp)] = 1\
            #aciton = q_values.argmax(1)[0]\
        return action\
\
    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\
        """ Compute td loss using torch operations only. Use the formula above. """\
        actions = torch.tensor(actions).long()    # shape: [batch_size]\
        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\
        is_done = torch.tensor(is_done, dtype = torch.uint8)  # shape: [batch_size]\
\
        if self.USE_CUDA:\
            actions = actions.cuda()\
            rewards = rewards.cuda()\
            is_done = is_done.cuda()\
\
        # get q-values for all actions in current states\
        predicted_qvalues = self.DQN(states.reshape(-1,state_channel))\
\
        # select q-values for chosen actions\
        x = actions.reshape(-1,action_space)#torch.tensor(range(1,16))\
        #x.requires_grad = True\
        predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]),:]*x\
        # predicted_qvalues_for_actions.requires_grad = True\
        predicted_qvalues_for_actions = torch.sum(predicted_qvalues_for_actions, dim=1)\
\
\
        #predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]),:].detach()\
        \
        # compute q-values for all actions in next states\
        ## Where DDQN is different from DQN\
        predicted_next_qvalues_current = self.DQN(next_states)\
        predicted_next_qvalues_target = self.DQN_target(next_states)\
        # compute V*(next_states) using predicted next q-values\
        next_state_values =  predicted_next_qvalues_target.gather(1, torch.max(predicted_next_qvalues_current, 1)[1].unsqueeze(1)).squeeze(1)\
\
        # compute "target q-values" for loss - it's what's inside square parentheses in the above formula.\
        target_qvalues_for_actions = rewards + gamma *next_state_values\
        \
        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\
        target_qvalues_for_actions = torch.where(\
            is_done, rewards, target_qvalues_for_actions)\
        \
        # target_qvalues_for_actions.require_grad = False\
        # mean squared error loss to minimize\
        #loss = torch.mean((predicted_qvalues_for_actions -\
        #                   target_qvalues_for_actions.detach()) ** 2)\
        # loss = F.smooth_l1_loss(torch.from_numpy(predicted_qvalues_for_actions), target_qvalues_for_actions.detach().reshape(predicted_qvalues_for_actions.shape))\
        #print(predicted_qvalues_for_actions - target_qvalues_for_actions.detach())\
        l = torch.nn.MSELoss(reduction='sum')\
        loss = l(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\
        #loss.requires_grad = True\
        return loss\
\
    def sample_from_buffer(self, batch_size):\
        states, actions, rewards, next_states, dones = [], [], [], [], []\
        for i in range(batch_size):\
            idx = random.randint(0, self.memory_buffer.size() - 1)\
            data = self.memory_buffer.buffer[idx]\
            frame, action, reward, next_frame, done= data\
            states.append(self.observe(frame))\
            actions.append(action)\
            rewards.append(reward)\
            next_states.append(self.observe(next_frame))\
            dones.append(done)\
        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\
\
    def learn_from_experience(self, batch_size):\
        if self.memory_buffer.size() > batch_size:\
            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\
            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\
            self.optimizer.zero_grad()\
            td_loss.backward()\
            #for param in self.DQN.parameters():\
                #param.grad.data.clamp_(-1, 1)\
\
            self.optimizer.step()\
            return(td_loss.item())\
        else:\
            return(0)\
\
def moving_average(a, n=3) :\
    ret = np.cumsum(a, dtype=float)\
    ret[n:] = ret[n:] - ret[:-n]\
    return ret[n - 1:] / n\
\
def plot_training(frame_idx, rewards, losses):\
    clear_output(True)\
    plt.figure(figsize=(20,5))\
    plt.subplot(131)\
    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-100:])))\
    plt.plot(rewards)\
    plt.subplot(132)\
    plt.title('loss, average on 100 stpes')\
    plt.plot(losses,linewidth=0.2)\
    plt.show()\
\
\pard\pardeftab720\partightenfactor0
\cf2 # if __name__ == '__main__':\
\
# Training DQN in PongNoFrameskip-v4\
\pard\pardeftab720\partightenfactor0
\cf2 env = gym.make('PathEnv-v1').unwrapped\
\
\
gamma = 0.99\
epsilon_max = 0.5\
epsilon_min = 0.01\
eps_decay = 30000\
frames = 50000\
\pard\pardeftab720\partightenfactor0
\cf2 USE_CUDA = False #True\
\pard\pardeftab720\partightenfactor0
\cf2 learning_rate = 1e-3\
max_buff = 10000\
update_tar_interval = 1000\
batch_size = 32\
print_interval = 1000\
log_interval = 1000\
learning_start = 10000\
win_reward = 18     # Pong-v4\
win_break = True\
\pard\pardeftab720\partightenfactor0
\cf2 N=1\
\pard\pardeftab720\partightenfactor0
\cf2 action_number = 4\
action_space = N*action_number\
\pard\pardeftab720\partightenfactor0
\cf2 # action_dim = 3\
# state_dim = env.observation_space.shape[0]\
\pard\pardeftab720\partightenfactor0
\cf2 state_channel = N*2\
agent = DDQNAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\
\
frame = env.reset()\
\
episode_reward = 0\
all_rewards = []\
losses = []\
episode_num = 0\
is_win = False\
\pard\pardeftab720\partightenfactor0
\cf2 # tensorboard\
\pard\pardeftab720\partightenfactor0
\cf2 summary_writer = SummaryWriter(log_dir = "DDQN", comment= "good_makeatari")\
\
\pard\pardeftab720\partightenfactor0
\cf2 # e-greedy decay\
\pard\pardeftab720\partightenfactor0
\cf2 epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\
            -1. * frame_idx / eps_decay)\
\pard\pardeftab720\partightenfactor0
\cf2 # plt.plot([epsilon_by_frame(i) for i in range(10000)])\
\pard\pardeftab720\partightenfactor0
\cf2 i = 0\
action_buffer = np.zeros(action_number)\
\pard\pardeftab720\partightenfactor0
\cf2 while episode_num<200:#for i in range(frames):\
    i+=1\
    epsilon = epsilon_by_frame(i)\
    state = agent.observe(frame)\
    action = agent.act(state.reshape(1,-1), epsilon)\
    while(np.linalg.norm(np.dot((action[0]+action_buffer),np.array([[0,1],[0,-1],[-1,0],[1,0]])))<1):\
        #print(action,action_buffer)\
        action = agent.act(state.reshape(1,-1), 1)\
    action_buffer=action[0]\
    next_frame, reward, done, _ = env.step(action)\
\
    episode_reward += reward\
    agent.memory_buffer.push(frame.reshape(-1,state_channel), action, reward, next_frame.reshape(-1,state_channel), done)\
    frame = next_frame\
\
    loss = 0\
    if agent.memory_buffer.size() >= learning_start:\
        loss = agent.learn_from_experience(batch_size)\
        losses.append(loss)\
\
\
    # if i % print_interval == 0:\
    #     print("frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d" % (i, episode_reward, loss, epsilon, episode_num))\
\
\
    if i % update_tar_interval == 0:\
        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\
\
    if done:\
\
        frame = env.reset()\
        all_rewards.append(episode_reward)\
        episode_reward = 0\
        episode_num += 1\
        print("frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d" % (i, np.mean(all_rewards[-1:]), loss, epsilon, episode_num))\
        #avg_reward = float(np.mean(all_rewards[-100:]))\
\
\pard\pardeftab720\partightenfactor0
\cf2 summary_writer.close()\
\pard\pardeftab720\partightenfactor0
\cf2 # 
\f1 \'b1\'a3\'b4\'e6\'cd\'f8\'c2\'e7\'b2\'ce\'ca\'fd
\f2 \
# torch.save(agent.DQN.state_dict(), "trained model/DDQN_dict.pth.tar")\
\pard\pardeftab720\partightenfactor0
\cf2 plot_training(i, all_rewards, losses)\
\pard\pardeftab720\partightenfactor0
\cf2 M = np.zeros([20,20])\
\pard\pardeftab720\partightenfactor0
\cf2 for i in range(20):\
    for j in range(20):\
        M[i,j] = agent.value(torch.tensor([[float(i),float(j)]])).detach().numpy().sum()\
\pard\pardeftab720\partightenfactor0
\cf2 # print(M)\
\
\pard\pardeftab720\partightenfactor0
\cf2 done = False\
frame = env.reset()\
\pard\pardeftab720\partightenfactor0
\cf2 while not done:\
    epsilon = 0\
    state = agent.observe(frame)\
    action = agent.act(state.reshape(1,-1), epsilon)\
    next_frame, reward, done, _ = env.step(action)\
    frame = next_frame\
    episode_reward += reward\
    print(action,frame,reward)\
\
env
\f1 \'a3\'ba
\f2 \
\pard\pardeftab720\partightenfactor0
\cf2 \cb3 \outl0\strokewidth0 \strokec14 import\strokec15  \strokec16 logging\strokec15 \
\strokec14 from\strokec15  \strokec16 importlib_metadata\strokec15  \strokec14 import\strokec15  \strokec16 re\strokec15 \
\strokec14 import\strokec15  \strokec16 numpy\strokec15  \strokec14 as\strokec15  \strokec16 np\strokec15 \
\strokec14 import\strokec15  \strokec16 gym\strokec15 \
\
\pard\pardeftab720\partightenfactor0
\cf2 \strokec17 logger\strokec15  = \strokec16 logging\strokec15 .\strokec18 getLogger\strokec15 (\strokec17 __name__\strokec15 )\
\
\pard\pardeftab720\partightenfactor0
\cf2 \strokec19 N\strokec15  = \strokec16 np\strokec15 .\strokec18 array\strokec15 ([[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ],\
[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ,\strokec20 0\strokec15 ]])\
\
\strokec19 M\strokec15  = \strokec16 np\strokec15 .\strokec18 zeros\strokec15 ([\strokec20 20\strokec15 ,\strokec20 20\strokec15 ],\strokec17 dtype\strokec15 =\strokec16 int\strokec15 )-\strokec20 1\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec21 # M[4,5] = 0\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec19 M\strokec15 [\strokec20 14\strokec15 ,\strokec20 10\strokec15 ] = \strokec20 1\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec21 # M[7,15] = 0\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec19 N\strokec15 =\strokec20 1\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec17 action_number\strokec15  = \strokec20 4\strokec15 \
\strokec17 action_space\strokec15  = \strokec19 N\strokec15 *\strokec17 action_number\strokec15 \
\strokec17 state_channel\strokec15  = \strokec19 N\strokec15 *\strokec20 2\strokec15 \
\pard\pardeftab720\partightenfactor0
\cf2 \strokec22 class\strokec15  \strokec16 PathEnv1\strokec15 (\strokec16 gym\strokec15 .\strokec16 Env\strokec15 ):\
    \strokec22 def\strokec15  \strokec18 __init__\strokec15 (\strokec17 self\strokec15 , \strokec17 render\strokec15  : \strokec16 bool\strokec15  = \strokec22 False\strokec15 ):\
        \strokec17 self\strokec15 .\strokec17 _render\strokec15  = \strokec17 render\strokec15 \
        \strokec21 # 
\f1 \'b6\'a8\'d2\'e5\'b6\'af\'d7\'f7\'bf\'d5\'bc\'e4
\f2 \strokec15 \
        \strokec17 self\strokec15 .\strokec17 action\strokec15  = \strokec16 np\strokec15 .\strokec18 zeros\strokec15 ([\strokec19 N\strokec15 ,\strokec17 action_number\strokec15 ],\strokec17 dtype\strokec15 =\strokec16 int\strokec15 )\
\
        \strokec21 # 
\f1 \'b6\'a8\'d2\'e5\'d7\'b4\'cc\'ac\'bf\'d5\'bc\'e4
\f2 \strokec15 \
        \strokec17 self\strokec15 .\strokec17 state\strokec15  = \strokec16 np\strokec15 .\strokec18 zeros\strokec15 ([\strokec19 N\strokec15 ,\strokec20 2\strokec15 ],\strokec17 dtype\strokec15 =\strokec16 int\strokec15 )\
\
        \strokec21 # 
\f1 \'b6\'a8\'d2\'e5\'bb\'d8\'b1\'a8
\f2 \strokec15 \
        \strokec17 self\strokec15 .\strokec17 r\strokec15  = \strokec20 0\strokec15 \
\
        \strokec21 # 
\f1 \'bc\'c6\'ca\'fd\'c6\'f7
\f2 \strokec15 \
        \strokec17 self\strokec15 .\strokec17 step_num\strokec15  = \strokec20 0\strokec15 \
    \
    \strokec22 def\strokec15  \strokec18 __apply_action\strokec15 (\strokec17 self\strokec15 , \strokec17 action\strokec15 ):\
        \strokec17 transMatrix\strokec15  = \strokec16 np\strokec15 .\strokec18 array\strokec15 ([[\strokec20 0\strokec15 ,\strokec20 1\strokec15 ],[\strokec20 0\strokec15 ,-\strokec20 1\strokec15 ],[-\strokec20 1\strokec15 ,\strokec20 0\strokec15 ],[\strokec20 1\strokec15 ,\strokec20 0\strokec15 ]])\
        \strokec17 self\strokec15 .\strokec17 state\strokec15  = \strokec17 self\strokec15 .\strokec17 state\strokec15  + \strokec16 np\strokec15 .\strokec18 dot\strokec15 (\strokec17 action\strokec15 ,\strokec17 transMatrix\strokec15 )\
        \strokec14 for\strokec15  \strokec17 s\strokec15  \strokec14 in\strokec15  \strokec17 self\strokec15 .\strokec17 state\strokec15 :\
            \strokec14 if\strokec15  \strokec18 min\strokec15 (\strokec17 s\strokec15 [\strokec20 0\strokec15 ],\strokec17 s\strokec15 [\strokec20 1\strokec15 ])<\strokec20 0\strokec15  \strokec22 or\strokec15  \strokec18 max\strokec15 (\strokec17 s\strokec15 [\strokec20 0\strokec15 ],\strokec17 s\strokec15 [\strokec20 1\strokec15 ])>\strokec20 19\strokec15 :\
                \strokec17 s\strokec15 [\strokec20 0\strokec15 ] = \strokec16 np\strokec15 .\strokec18 clip\strokec15 (\strokec17 s\strokec15 [\strokec20 0\strokec15 ],\strokec20 0\strokec15 ,\strokec20 19\strokec15 )\
                \strokec17 s\strokec15 [\strokec20 1\strokec15 ] = \strokec16 np\strokec15 .\strokec18 clip\strokec15 (\strokec17 s\strokec15 [\strokec20 1\strokec15 ],\strokec20 0\strokec15 ,\strokec20 19\strokec15 )\
                \strokec17 self\strokec15 .\strokec17 r\strokec15  += -\strokec20 100\strokec15 \
    \
\
    \strokec22 def\strokec15  \strokec18 reset\strokec15 (\strokec17 self\strokec15 ):\
        \strokec17 self\strokec15 .\strokec17 state\strokec15  = \strokec16 np\strokec15 .\strokec18 array\strokec15 ([[\strokec20 0\strokec15 ,\strokec20 0\strokec15 ]]) \strokec21 #np.array([[0,0],[19,0],[0,19]])\strokec15 \
        \strokec17 self\strokec15 .\strokec17 r\strokec15  = \strokec20 0\strokec15 \
        \strokec17 self\strokec15 .\strokec17 step_num\strokec15  = \strokec20 0\strokec15 \
        \strokec14 return\strokec15  \strokec17 self\strokec15 .\strokec17 state\strokec15 \
\
    \
    \strokec22 def\strokec15  \strokec18 reward\strokec15 (\strokec17 self\strokec15 ):\
        \strokec17 r\strokec15  = \strokec20 0\strokec15 \
        \strokec14 for\strokec15  \strokec17 s\strokec15  \strokec14 in\strokec15  \strokec17 self\strokec15 .\strokec17 state\strokec15 :\
            \strokec17 r\strokec15  += \strokec19 M\strokec15 [\strokec17 s\strokec15 [\strokec20 0\strokec15 ],\strokec17 s\strokec15 [\strokec20 1\strokec15 ]]\strokec21 # - np.linalg.norm(np.array([14,10])-s)\strokec15 \
        \strokec17 self\strokec15 .\strokec17 r\strokec15  += \strokec17 r\strokec15 \
\
    \strokec22 def\strokec15  \strokec18 judge\strokec15 (\strokec17 self\strokec15 ):\
        \strokec14 if\strokec15  \strokec17 self\strokec15 .\strokec17 step_num\strokec15  < \strokec20 2000\strokec15 :\
            \strokec14 if\strokec15  \strokec18 abs\strokec15 (\strokec16 np\strokec15 .\strokec18 array\strokec15 ([\strokec20 14\strokec15 ,\strokec20 10\strokec15 ])-\strokec17 self\strokec15 .\strokec17 state\strokec15 [\strokec20 0\strokec15 ]).sum() == \strokec20 0\strokec15 :\
                \strokec18 print\strokec15 (\strokec23 "state: "\strokec15 ,\strokec17 self\strokec15 .\strokec17 state\strokec15 )\
                \strokec14 return\strokec15  \strokec22 True\strokec15 \
                \strokec14 if\strokec15  np.array([\strokec20 4\strokec15 ,\strokec20 5\strokec15 ]) \strokec22 in\strokec15  \strokec22 self\strokec15 .state:\
                   \strokec14 if\strokec15  np.array([\strokec20 7\strokec15 ,\strokec20 15\strokec15 ]) \strokec22 in\strokec15  \strokec22 self\strokec15 .state:\
                       \strokec14 return\strokec15  \strokec22 True\strokec15 \
            \strokec14 return\strokec15  \strokec22 False\strokec15 \
        \strokec14 return\strokec15  \strokec22 True\strokec15 \
    \
    \strokec22 def\strokec15  \strokec18 step\strokec15 (\strokec17 self\strokec15 , \strokec17 action\strokec15 ):\
        \strokec17 self\strokec15 .\strokec17 r\strokec15  = \strokec20 0\strokec15 \
        \strokec17 self\strokec15 .\strokec18 __apply_action\strokec15 (\strokec17 action\strokec15 )\
        \strokec17 self\strokec15 .\strokec17 step_num\strokec15  += \strokec20 1\strokec15 \
        \strokec17 state\strokec15  = \strokec17 self\strokec15 .\strokec17 state\strokec15 \
        \strokec17 self\strokec15 .\strokec18 reward\strokec15 ()\
        \strokec14 if\strokec15  \strokec17 self\strokec15 .\strokec18 judge\strokec15 ():\
            \strokec17 done\strokec15  = \strokec22 True\strokec15 \
        \strokec14 else\strokec15 :\
            \strokec17 done\strokec15  = \strokec22 False\strokec15 \
        \strokec17 info\strokec15  = \{\}\
        \strokec14 return\strokec15  \strokec17 state\strokec15 , \strokec17 self\strokec15 .\strokec17 r\strokec15 , \strokec17 done\strokec15 , \strokec17 info\strokec15 \
    \
    \strokec22 def\strokec15  \strokec18 seed\strokec15 (\strokec17 self\strokec15 , \strokec17 seed\strokec15 =\strokec22 None\strokec15 ):\
        \strokec14 pass\strokec15 \
\
    \strokec22 def\strokec15  \strokec18 render\strokec15 (\strokec17 self\strokec15 , \strokec17 mode\strokec15 =\strokec23 'human'\strokec15 ):\
        \strokec14 pass\strokec15 \
}